{"cells":[{"cell_type":"markdown","metadata":{"id":"U5xRK-zKl9eD"},"source":["#Downlaod the libraries and requirements\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oiypv3OcQZ19","outputId":"b404bcee-7aa3-4728-d77f-e8dfc6e314df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n","Collecting pip\n","  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n","Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n"]}],"source":["!rm -rf sample_data\n","!rm -rf tmp\n","\n","!pip install -U pip\n","\n","# # Ensure Ninja is installed\n","# !conda install Ninja\n","\n","# # Install the correct version of CUDA\n","# !conda install cuda -c nvidia/label/cuda-12.1.0\n","\n","# Install PyTorch and xformers\n","\n","# You may need to install another xformers version if you use a different PyTorch version\n","!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n","!pip install xformers==0.0.22.post7\n","\n","# # For Linux users: Install Triton\n","# !pip install triton\n","!pip install transformers datasets\n","\n","!pip install huggingface_hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uK8cGhPdQZOU"},"outputs":[],"source":["%cd /content\n","!GIT_LFS_SKIP_SMUDGE=1 git clone -b dev https://github.com/camenduru/InstantMesh\n","%cd /content/InstantMesh\n","\n","# Install other requirements\n","!pip install -r /content/InstantMesh/requirements.txt\n","!pip install pytorch-lightning==2.1.2 gradio==3.50.2 einops omegaconf torchmetrics webdataset accelerate tensorboard\n","!pip install PyMCubes trimesh rembg transformers diffusers==0.20.2 bitsandbytes imageio[ffmpeg] xatlas plyfile\n","!pip install git+https://github.com/NVlabs/nvdiffrast jax==0.4.19 jaxlib==0.4.19 ninja"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28559,"status":"ok","timestamp":1724152145564,"user":{"displayName":"Muhammad Hijazi","userId":"04451631093774096322"},"user_tz":-180},"id":"X3uOMHACQcHU","outputId":"e75fef55-725d-430c-e4c4-98c9efca47b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/InstantMesh\n"]}],"source":["%cd /content/InstantMesh\n","\n","import numpy as np\n","import rembg\n","from PIL import Image\n","from pytorch_lightning import seed_everything\n","from einops import rearrange\n","from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler\n","from huggingface_hub import hf_hub_download\n","from src.utils.infer_util import remove_background, resize_foreground"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1724152145564,"user":{"displayName":"Muhammad Hijazi","userId":"04451631093774096322"},"user_tz":-180},"id":"Thv8qWneQm0c","outputId":"884ada43-a0dd-4591-d321-85a60073afd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["%cd /content/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpQ7WfxXQo6z"},"outputs":[],"source":["# !huggingface-cli login\n","# from datasets import load_dataset\n","# ds = load_dataset(\"Arkan0ID/furniture-dataset\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcdAPU7gQqnU"},"outputs":[],"source":["# ds.save_to_disk(\"furniture-dataset\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JlWKiMuqRFU0"},"outputs":[],"source":["# from datasets import load_from_disk\n","# ds = load_from_disk(\"furniture-dataset\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJMXKOy0QtGk"},"outputs":[],"source":["# print(ds.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1sqfumMQxNo"},"outputs":[],"source":["# Clear apt cache\n","!apt-get clean\n","\n","# Clear pip cache\n","!rm -rf ~/.cache/pip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q0HzjjmvQ-Sz"},"outputs":[],"source":["# import gc\n","# gc.collect()\n","# torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"VPq8kSOomY58"},"source":["#Downlaod the pre-trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"huLnnxiXRO_U"},"outputs":[],"source":["# from huggingface_hub import hf_hub_download\n","\n","# # Specify the model repository and filename\n","# model_repo = \"TencentARC/InstantMesh\"\n","# filename = \"instant_mesh_large.ckpt\"\n","\n","# # Download the file\n","# ckpt_path = hf_hub_download(repo_id=model_repo, filename=filename)\n","\n","# print(f\"Downloaded file saved at: {ckpt_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38896,"status":"ok","timestamp":1724152184442,"user":{"displayName":"Muhammad Hijazi","userId":"04451631093774096322"},"user_tz":-180},"id":"6pMvd5f1RQkT","outputId":"583f63b7-0366-4e23-e01f-88d4e327e55b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Download complete!\n"]}],"source":["import requests\n","\n","# The direct URL to the ckpt file\n","model_file_url = 'https://huggingface.co/TencentARC/InstantMesh/resolve/main/instant_mesh_large.ckpt'\n","\n","# Download the file\n","response = requests.get(model_file_url)\n","\n","# Save the file\n","with open(\"instant_mesh_large.ckpt\", \"wb\") as file:\n","    file.write(response.content)\n","\n","print(\"Download complete!\")"]},{"cell_type":"markdown","metadata":{"id":"ArTgU8AXmfVn"},"source":["#Check the size of downloaded pre-trained"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1724152184442,"user":{"displayName":"Muhammad Hijazi","userId":"04451631093774096322"},"user_tz":-180},"id":"7l0DXpr1RSPr","outputId":"3b467c77-c2ee-4761-a75e-ca2e6b76eb78"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.5G\tinstant_mesh_large.ckpt\n"]}],"source":["!du -sh instant_mesh_large.ckpt"]},{"cell_type":"markdown","metadata":{"id":"kkho9Xz3moxc"},"source":["#Convert model downlaod to pth file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17106,"status":"ok","timestamp":1724152201537,"user":{"displayName":"Muhammad Hijazi","userId":"04451631093774096322"},"user_tz":-180},"id":"ltB0Neg4Rbml","outputId":"21ad8cfd-c30b-4868-d4d4-52c52cfea2f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model state_dict saved to instant_mesh_large.pth\n"]}],"source":["import torch\n","\n","# Load the .ckpt file\n","ckpt_path = \"instant_mesh_large.ckpt\"\n","loaded_ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n","\n","# If the loaded checkpoint has a model state_dict, extract it\n","if 'state_dict' in loaded_ckpt:\n","    model_state_dict = loaded_ckpt['state_dict']\n","else:\n","    model_state_dict = loaded_ckpt\n","\n","# Save the state_dict to a .pth file\n","save_path = ckpt_path.replace(\".ckpt\", \".pth\")\n","torch.save(model_state_dict, save_path)\n","\n","print(f\"Model state_dict saved to {save_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1724152201537,"user":{"displayName":"Muhammad Hijazi","userId":"04451631093774096322"},"user_tz":-180},"id":"npccRBw2Rduz","outputId":"17e399f0-d79b-467b-b7f6-0e64b362e2a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.5G\tinstant_mesh_large.pth\n"]}],"source":["!du -sh instant_mesh_large.pth"]},{"cell_type":"markdown","metadata":{"id":"b94W90n3RVrW"},"source":["#Custimize the dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["af396967374c4594a3601775d3f621c3","78fb8b22e5ee48d68f5b376684648ae8","1c9964ec7f9e4841ac42bd04b2eb283e","67b53be3cae8432fb62c142aacb5a96c","676717d2715046ec884c99721037e771","6f4ca10ed51343a2a3b24cb961e355b0","c29290e7dac34b6db33469f5a7e6a433","a348ceec9d394e8a9bb73b396f94ec54","645d45f9c66345878fd629628ae512cd","cd862b16e20940b78f334cfeb32ec73d","ecfbe27f6f0142dd946874a5c90fc55e"]},"executionInfo":{"elapsed":26990,"status":"ok","timestamp":1724152228516,"user":{"displayName":"Muhammad Hijazi","userId":"04451631093774096322"},"user_tz":-180},"id":"4tFteASURUou","outputId":"22e62a22-f722-44a8-b6ce-ecc9033228f2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Resolving data files:   0%|          | 0/1493 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af396967374c4594a3601775d3f621c3"}},"metadata":{}}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from datasets import load_dataset\n","\n","# Load the dataset from Hugging Face\n","ds = load_dataset(\"Arkan0ID/furniture-dataset\")\n","\n","# Transformation to resize images, convert to tensor, and ensure 3 channels\n","transform = transforms.Compose([\n","    transforms.Resize((320, 320)),\n","    transforms.ToTensor(),\n","    transforms.Lambda(lambda image: image.repeat(3, 1, 1) if image.shape[0] == 1 else image)\n","])\n","\n","# Custom Dataset class to apply transformations and return labels\n","class CustomDataset(Dataset):\n","    def __init__(self, hf_dataset, transform=None):\n","        self.hf_dataset = hf_dataset\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.hf_dataset)\n","\n","    def __getitem__(self, idx):\n","        image = self.hf_dataset[idx]['image']\n","        label = self.hf_dataset[idx]['label']  # Adjust based on your dataset\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label  # Return both image and label\n","\n","# Determine the number of classes based on the dataset\n","num_classes = len(set([item['label'] for item in ds['train']]))\n","\n","# Shard the dataset for a smaller subset to avoid memory issues\n","train_data = ds['train'].shard(num_shards=5, index=0)  # 1/5th of the data for training\n","val_data = ds['train'].shard(num_shards=5, index=1)    # 1/5th of the data for validation\n","\n","# Create DataLoaders for the training and validation datasets\n","train_loader = DataLoader(CustomDataset(train_data, transform=transform), batch_size=32, shuffle=True, pin_memory=True, num_workers=2)\n","val_loader = DataLoader(CustomDataset(val_data, transform=transform), batch_size=32, shuffle=False, pin_memory=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"8fH4XYsVm491"},"source":["#Define the model architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFaC-KVCRjiu"},"outputs":[],"source":["# import torch\n","# import torch.nn as nn\n","\n","# # Define the model architecture\n","# class InstantMeshModel(nn.Module):\n","#     def __init__(self, num_classes):\n","#         super(InstantMeshModel, self).__init__()\n","#         # Define layers based on the architecture found in the checkpoint\n","#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","#         self.bn1 = nn.BatchNorm2d(64)\n","#         self.relu = nn.ReLU(inplace=True)\n","#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","#         # ... other layers as needed ...\n","\n","#         # Add layers to transform the output to the correct shape for the fully connected layer\n","#         self.conv2 = nn.Conv2d(64, 512, kernel_size=3, padding=1) # Example layer, adjust as needed\n","#         self.bn2 = nn.BatchNorm2d(512)\n","\n","#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","#         self.fc = nn.Linear(512, num_classes)\n","\n","#     def forward(self, x):\n","#         x = self.conv1(x)\n","#         x = self.bn1(x)\n","#         x = self.relu(x)\n","#         x = self.maxpool(x)\n","\n","#         x = self.conv2(x) # Example layer, adjust as needed\n","#         x = self.bn2(x)\n","#         x = self.relu(x)\n","\n","#         x = self.avgpool(x)\n","#         x = torch.flatten(x, 1)\n","#         x = self.fc(x)\n","#         return x\n","\n","# # Initialize the model\n","# num_classes = len(set([item['label'] for item in ds['train']])) # Calculate num_classes from the dataset\n","# model = InstantMeshModel(num_classes=num_classes)\n","\n","# # Load the state dict into the model\n","# model_state_dict = torch.load('/content/instant_mesh_large.pth', map_location=torch.device('cpu'))\n","# model.load_state_dict(model_state_dict, strict=False)  # Use strict=False if the model doesn't match exactly\n","\n","# # Unfreeze layers for fine-tuning, if required\n","# for param in model.fc.parameters():\n","#     param.requires_grad = True"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import StepLR\n","from torchvision import transforms\n","\n","# Define the model architecture\n","class InstantMeshModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(InstantMeshModel, self).__init__()\n","        # Define layers\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        # Additional layers\n","        self.conv2 = nn.Conv2d(64, 512, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(512)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512, num_classes)\n","\n","        # Initialize weights\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","# Data preprocessing with augmentations\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ]),\n","}\n","\n","# Initialize the model\n","num_classes = len(set([item['label'] for item in ds['train']]))  # Calculate num_classes from the dataset\n","model = InstantMeshModel(num_classes=num_classes)\n","\n","# Load the state dict into the model\n","model_state_dict = torch.load('/content/instant_mesh_large.pth', map_location=torch.device('cpu'))\n","model.load_state_dict(model_state_dict, strict=False)  # Use strict=False if the model doesn't match exactly\n","\n","# Freeze all layers except the final fully connected layer\n","for param in model.parameters():\n","    param.requires_grad = False\n","for param in model.fc.parameters():\n","    param.requires_grad = True\n","\n","# Define optimizer and learning rate scheduler\n","optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n","scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","# Mixed precision training\n","scaler = torch.cuda.amp.GradScaler()\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    scheduler.step()\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"76qqxR1nrSgi","executionInfo":{"status":"error","timestamp":1724152253119,"user_tz":-180,"elapsed":24616,"user":{"displayName":"Muhammad Hijazi","userId":"04451631093774096322"}},"outputId":"7a856fdf-08c9-42c9-9836-6a4294d1aab6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'num_epochs' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-576180d86cbf>\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"JsfvHbREm8CC"},"source":["#Train pre-trained model  (Fine-tune)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NieHJkenRk-c"},"outputs":[],"source":["# import torch.optim as optim # Import the optimizer module\n","# # Define loss function and optimizer\n","# loss_fn = nn.CrossEntropyLoss()\n","# optimizer = optim.Adam(model.parameters(), lr=1e-5)\n","\n","# # Training function remains the same\n","# def train_model(model, train_loader, val_loader, epochs=500):\n","#     for epoch in range(epochs):\n","#         model.train()\n","#         for images, labels in train_loader:\n","#             optimizer.zero_grad()\n","#             outputs = model(images)\n","#             loss = loss_fn(outputs, labels)\n","#             loss.backward()\n","#             optimizer.step()\n","\n","#         # Evaluate on validation set\n","#         model.eval()\n","#         val_loss = 0\n","#         with torch.no_grad():\n","#             for images, labels in val_loader:\n","#                 outputs = model(images)\n","#                 val_loss += loss_fn(outputs, labels).item()\n","#         val_loss /= len(val_loader.dataset)\n","#         print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n","\n","# # Train the model\n","# train_model(model, train_loader, val_loader)"]},{"cell_type":"markdown","metadata":{"id":"tfG2iSh7nHzG"},"source":["#Save the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kKhye4URqhT"},"outputs":[],"source":["torch.save(model.state_dict(), 'finetuned_instantmesh.pth')"]},{"cell_type":"markdown","metadata":{"id":"XkL47p_AnKgX"},"source":["#Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QqlJ89TsRr4M"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Assuming 'val_loader' from your training code contains validation images\n","validation_images = []\n","for images, _ in val_loader:\n","    validation_images.extend(images)\n","    if len(validation_images) >= 5:  # Collect only 5 images\n","        break\n","\n","model.eval()\n","with torch.no_grad():\n","    for i, image in enumerate(validation_images[:5]):\n","        output = model(image.unsqueeze(0))\n","\n","        # Get the predicted class\n","        _, predicted = torch.max(output, 1)\n","\n","        plt.subplot(2, 5, i+1)\n","        plt.imshow(image.permute(1, 2, 0))  # Show input image\n","        plt.title(\"Input Image\")\n","\n","        plt.subplot(2, 5, i+6)\n","        # Display the predicted class or a representation of the output\n","        plt.text(0.5, 0.5, f\"Predicted: {predicted.item()}\", ha='center', va='center', fontsize=10)\n","        plt.axis('off')  # Turn off axes for text display\n","        plt.title(\"Prediction\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDZPjMtAs0Dz"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","import torch\n","\n","# Assuming 'val_loader' from your training code contains validation images\n","validation_images, labels = next(iter(val_loader))\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Initialize a figure with a larger size\n","plt.figure(figsize=(15, 6))\n","\n","# Process images in a batch\n","with torch.no_grad():\n","    # Get predictions for the batch\n","    outputs = model(validation_images[:5])\n","\n","    # Get predicted classes\n","    _, predicted = torch.max(outputs, 1)\n","\n","    for i in range(5):\n","        image = validation_images[i]\n","        pred = predicted[i].item()\n","        true_label = labels[i].item()\n","\n","        # Convert the tensor to a PIL image for correct display\n","        img = transforms.ToPILImage()(image)\n","\n","        # Display the input image\n","        plt.subplot(2, 5, i+1)\n","        plt.imshow(img)\n","        plt.title(f\"Input Image\\nTrue: {true_label}\")\n","        plt.axis('off')  # Remove axes for better clarity\n","\n","        # Display the predicted label\n","        plt.subplot(2, 5, i+6)\n","        plt.text(0.5, 0.5, f\"Predicted: {pred}\", ha='center', va='center', fontsize=14, color='blue')\n","        plt.axis('off')\n","        plt.title(\"Prediction\")\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"vPT5KY2enOR9"},"source":["#Codes may need"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJgs_pZvR0dT"},"outputs":[],"source":["# import torch\n","# import torch.nn as nn\n","\n","# # Define the model architecture\n","# class InstantMeshModel(nn.Module):\n","#     def __init__(self, num_classes):\n","#         super(InstantMeshModel, self).__init__()\n","#         # Define layers based on the architecture found in the checkpoint\n","#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","#         self.bn1 = nn.BatchNorm2d(64)\n","#         self.relu = nn.ReLU(inplace=True)\n","#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","#         # ... other layers as needed ...\n","#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # Add an adaptive average pooling layer\n","#         self.fc = nn.Linear(512, num_classes)  # Adjust the input features to match the output of previous layers\n","\n","#     def forward(self, x):\n","#         x = self.conv1(x)\n","#         x = self.bn1(x)\n","#         x = self.relu(x)\n","#         x = self.maxpool(x)\n","#         # ... forward pass through other layers ...\n","#         x = self.avgpool(x) # Apply adaptive average pooling to get a (batch_size, 512, 1, 1) tensor\n","#         x = torch.flatten(x, 1) # Flatten the tensor to (batch_size, 512)\n","#         x = self.fc(x)\n","#         return x\n","# # # Define the model architecture\n","# # class InstantMeshModel(nn.Module):\n","# #     def __init__(self, num_classes):\n","# #         super(InstantMeshModel, self).__init__()\n","# #         # Define layers based on the architecture found in the checkpoint\n","# #         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","# #         self.bn1 = nn.BatchNorm2d(64)\n","# #         self.relu = nn.ReLU(inplace=True)\n","# #         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","# #         # More layers to match the architecture\n","# #         self.fc = nn.Linear(2048, num_classes)  # Adjust according to the architecture\n","\n","# #     def forward(self, x):\n","# #         x = self.conv1(x)\n","# #         x = self.bn1(x)\n","# #         x = self.relu(x)\n","# #         x = self.maxpool(x)\n","# #         # Continue through the layers\n","# #         x = self.fc(x)\n","# #         return x\n","\n","# # Initialize the model\n","# num_classes = 10  # Replace with your actual number of classes\n","# model = InstantMeshModel(num_classes=num_classes)\n","\n","# # Load the state dict into the model\n","# model_state_dict = torch.load('/content/InstantMesh/instant_mesh_large.pth', map_location=torch.device('cpu'))\n","# model.load_state_dict(model_state_dict, strict=False)  # Use strict=False if the model doesn't match exactly\n","\n","# # Unfreeze layers for fine-tuning, if required\n","# for param in model.fc.parameters():\n","#     param.requires_grad = True\n","\n","# # Now the model is ready to be trained or used for inference\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9MCvW08R5nV"},"outputs":[],"source":["# !apt-get install aria2\n","# #instant_mesh_large.ckpt\n","# # access_token = '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'\n","\n","# model_file_url = 'https://huggingface.co/TencentARC/InstantMesh/blob/main/instant_mesh_large.ckpt?download=true'\n","\n","# # Download the file using aria2c\n","# !aria2c --header=\"Authorization: Bearer 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\" {model_file_url}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvc8W2P-RnZg"},"outputs":[],"source":["# with open('/content/instant_mesh_large.ckpt', 'rb') as f:\n","#     print(f.read(100))  # Read and print the first 100 bytes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oH8ZjbbCJDjj"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"af396967374c4594a3601775d3f621c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78fb8b22e5ee48d68f5b376684648ae8","IPY_MODEL_1c9964ec7f9e4841ac42bd04b2eb283e","IPY_MODEL_67b53be3cae8432fb62c142aacb5a96c"],"layout":"IPY_MODEL_676717d2715046ec884c99721037e771"}},"78fb8b22e5ee48d68f5b376684648ae8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f4ca10ed51343a2a3b24cb961e355b0","placeholder":"​","style":"IPY_MODEL_c29290e7dac34b6db33469f5a7e6a433","value":"Resolving data files: 100%"}},"1c9964ec7f9e4841ac42bd04b2eb283e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a348ceec9d394e8a9bb73b396f94ec54","max":1493,"min":0,"orientation":"horizontal","style":"IPY_MODEL_645d45f9c66345878fd629628ae512cd","value":1493}},"67b53be3cae8432fb62c142aacb5a96c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd862b16e20940b78f334cfeb32ec73d","placeholder":"​","style":"IPY_MODEL_ecfbe27f6f0142dd946874a5c90fc55e","value":" 1493/1493 [00:00&lt;00:00,  3.83it/s]"}},"676717d2715046ec884c99721037e771":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f4ca10ed51343a2a3b24cb961e355b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c29290e7dac34b6db33469f5a7e6a433":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a348ceec9d394e8a9bb73b396f94ec54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"645d45f9c66345878fd629628ae512cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd862b16e20940b78f334cfeb32ec73d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecfbe27f6f0142dd946874a5c90fc55e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}